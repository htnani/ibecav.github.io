---
title: "CHAID & R: When you need explanation -- May 15, 2018"
author: "Chuck  Powell"
date: "5/15/2018"
output: github_document
tags: R ggplot2 dplyr CHAID caret purrr
---

A modern data scientist using R has access to an almost bewildering number of tools, libraries and algorithms to analyze the data.  In my next two posts I'm going to focus on an in depth visit with CHAID (Chi-square automatic interaction detection).  The title should give you a hint for why I think CHAID is a good "tool" for your analytical toolbox.  There are lots of tools that can help you predict or classify but CHAID is especially good at helping you explain to **any audience** how the model arrives at it's prediction or classification.  It's also incredibly robust from a statistical perspective, making almost no assumptions about your data for distribution or normality.  I'll try and elaborate on that as we work the example.

You can get a very brief summary of CHAID from  [wikipedia](https://en.wikipedia.org/wiki/Chi-square_automatic_interaction_detection) and mentions of it scattered about in places like [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/) or [Data Flair](https://data-flair.training/blogs/r-decision-trees/). If you prefer a more scholarly bent the original article can be found in places like [JSTOR](https://www.jstor.org/stable/2986296).  As the name implies it is fundamentally based on the venerable Chi-square test -- and while not the most powerful (in terms of detecting the smallest possible differences) or the fastest, it really is easy to manage and more importantly to tell the story after using it.

Compared to some other techniques it's also quite simple to use, as I hope you'll agree, by the end of these posts.  To showcase it we're going to be using a dataset that comes to us from the [IBM Watson Project](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) and comes packaged with the `rsample` library.  It's a very practical and understandable dataset.  A great use case for a tree based algorithm.  Imagine yourself in a fictional company faced with the task of trying to figure out which employees you are going to "lose" a.k.a. attrition or turnover.  There's a steep cost involved in keeping good employees and training and on-boarding can be expensive.  Being able to predict attrition even a little bit better would save you lots of money and make the company better, especially if you can understand exactly what you have to "watch out" for that might indicate the person is a high risk to leave.

## Setup and library loading

If you've never used `CHAID` before you may also not have `partykit`.  `CHAID` isn't on `CRAN` but I have commented out the install command below.  You'll also get a variety of messages, none of which is relevant to this example so I've suppressed them.

```{r setup, warning = FALSE, message = FALSE, echo=TRUE}
# install.packages("partykit")
# install.packages("CHAID", repos="http://R-Forge.R-project.org")
require(rsample) # for dataset and splitting also loads broom and tidyr
require(dplyr)
require(ggplot2)
theme_set(theme_bw()) # set theme
require(CHAID)
require(purrr)
require(caret)
```

## Predicting attrition in a fictional company

Let's load up the `attrition` dataset and take a look at the variables we have.

```{r best1}
# data(attrition)
str(attrition)
```

Okay we have data on 1,470 employees.  We have 30 potential predictor or independent variables and the all important `attrition` variable which gives us a yes or no answer to the question of whether or not the employee left.  We're to build the most accurate predictive model we can that is also simple (parsimonious) and explainable. The predictors we have seem to be the sorts of data we might have on hand in our HR files and thank goodness are labelled in a way that makes them pretty self explanatory.

The `CHAID` library in `R` requires that any variables that we enter as predictors be either nominal or ordinal variables (see `?CHAID::chaid`), which in R speak means we have to get them in as either `factor` or `ordered factor`.  The `str` command shows we have a bunch of variables which are of type `integer`.  As it turns out moving from integer to factor is simple in terms of code but has to be thoughtful for substantive reasons. So let's see how things breakdown.

```{r best2}
attrition %>%
  select_if(is.factor) %>%
  ncol
attrition %>%
  select_if(is.numeric) %>%
  ncol
```

Hmmmm, 15 factors and 16 integers.  Let's explore further. Of the variables that are integers how many of them have a small number of values (a.k.a. levels) and can therefore be simply and easily converted to true factors. We'll use a `dplyr pipe` to see how many have 5 or fewer levels and 10 or fewer levels.

```{r best3}
attrition %>%
  select_if(function(col)
    length(unique(col)) <= 5 & is.integer(col)) %>%
  head

attrition %>%
  select_if(function(col)
    length(unique(col)) <= 10 & is.integer(col)) %>%
  head
```

2 and 4 respectively. We can be pretty confident that converting these from `integer` to `factor` won't lose much information. Simple to run a `mutate` operation across the 4 we have identified. Probably more elegant though to make it a `mutate_if`.  That way in the future we decide we like 4 or 7 or 122 as our criteria for the change we only have to change one number. The "if" variation is also less to type and less likely to make a manual mistake. 

```{r best4}
attrition %>%
  mutate(
    JobLevel = factor(JobLevel),
    NumCompaniesWorked = factor(NumCompaniesWorked),
    StockOptionLevel = factor(StockOptionLevel),
    TrainingTimesLastYear = factor(TrainingTimesLastYear)
  ) %>% 
  str

attrition <- attrition %>% 
  mutate_if(function(col) length(unique(col)) <= 10 & is.integer(col), as.factor)

summary(attrition)
```

As you look at the results this is a good time to remind you that `CHAID` is "non parametric" which means that we don't have to worry about how the distribution (normality) looks nor make any assumptions about the variance. We are assuming that the predictors are independent of one another, but that is true of every statistical test and this is a robust procedure.  So for now, let's simply ignore all the variables that are still integers.  I promise we'll come back and deal with them later.  But for now I'm eager to actually use CHAID and do some predicting. We're also going to defer and address the issue of "over-fitting" and how to most wisely use the data we have.  We're simply going to build a first model using all 1,470 cases, the 18 factors we have available to predict with and we are trying to predict attrition.  We'll create a new dataframe called `newattrit` (how original right?).

```{r best5}
newattrit <- attrition %>% 
  select_if(is.factor)
dim(newattrit)
```

The `chaid` command accepts two pieces of information in it's simplest case, a formula like `outcome ~ predictors` and a dataframe.  We're going to make use of the `~ .` shortcut on the right hand side and add `attrition` on the left and `newattrit` as our dataframe.

About 6 seconds later (at least on my Mac) we'll have a solution that we can `print` and `plot`.

>   **I'm going to output all the plots in a smaller size for the benefit of you the readers.  I'm doing that via RMarkdown and it won't happen automatically for you if you download and use the code.  I'll initially be using, fig.height=10, fig.width=20, dpi=90, out.width="900px"** 

What does CHAID do?  Straight from the help pages "Select the predictor that has the smallest adjusted p-value (i.e., most significant). If this adjusted p-value is less than or equal to a user-specified alpha-level alpha4, split the node using this predictor. Else, do not split and the node is considered as a terminal node."  So it will take our 18 predictors and test each one against our outcome variable -- attrition.  The one with the lowest p value (a proxy for is most predictive) will "anchor" our decision tree.  It will then repeat this process of splitting until more splits fail to yield *significant* results.  I'm way over-simplifying, of course, but you get the idea.  The end result will be a series of `terminal nodes` (think of them as "prediction buckets" that have a group of employees who all meet the same criteria who we think will either attrit or not attrit).  Let's run it.

```{r best6, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
# demonstrate a full model using chaid with defaults
chaidattrit1 <- chaid(Attrition ~ ., data = newattrit)
print(chaidattrit1)
plot(chaidattrit1)
chisq.test(newattrit$Attrition, newattrit$OverTime)
```

I happen to be a visual learner and prefer the `plot` to the `print` but they are obviously reporting the same information so use them as you see fit.  As you can see the very first split it decides on is overtime yes or no.  I've run the chi-square test so that you can see the `p value` is indeed very small (0.00000000000000022).

So the algorithm has decided that the most predictive way to divide our sample of employees is into 20 terminal nodes or buckets.  Each one of the nodes represents a distinct set of predictors.  Take a minute to look at node 19.  Every person there shares the following characteristics.  

* [2] OverTime in No
* [15] StockOptionLevel in 1, 2, 3
* [17] EnvironmentSatisfaction in Medium, High, Very_High
* [19] Department in Research_Development: No

There are n = 314 in this group, our prediction is that `No` they will not attrit and we were "wrong" err = 3.2%.  That's some useful information. To quote an old Star Wars movie "These are not the droids you're looking for...".  In other words, this is not a group we should be overly worried about losing and we can say that with pretty high confidence.

For contrast let's look at node #23:

* [20] OverTime in Yes
* [21] JobLevel in 1
* [22] StockOptionLevel in 0, 3
* [23] JobSatisfaction in Low, Medium, High: 

Where there are n = 61 staff, we predict they will leave `Yes` and we get it wrong err = 26.2% of the time.  A little worrisome that we're not as accurate but this is a group that bears watching or intervention if we want to retain them.

Some other things to note. Because the predictors are considered categorical we will get splits like we do for node 22, where 0 and 3 are on one side and 1, 2 is on the other.  The number of people in any node can be quite variable. Finally, notice that a variable can occur at different levels of the model like `StockOptionLevel` does!

On the `plot` side of things there are a few key options you can adjust to make things easier to read.  The next blocks of code show you how to adjust some key options such as adding a title, reducing the font size, using "simple" mode, and changing colors.

```{r best7, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
# digress for plotting
plot(chaidattrit1, type = "simple")
plot(
  chaidattrit1,
  main = "Testing Graphical Options",
  gp = gpar(fontsize = 8),
  type = "simple"
)
plot(
  chaidattrit1,
  main = "Testing More Graphical Options",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3,
    fontsize = 10
  )
)
```

## Exercising some control

Next let's look into varying the parameters `chaid` uses to build the model.  `chaid_control` (not surprisingly) controls the behavior of the model building.  When you check the documentation at `?chaid_control` you can see the list of 8 parameters you can adjust.  We've already run the default settings implicitly when we built `chaidattrit1` let's look at three others.

* `minsplit` - Number of observations in splitted response at which no further split is desired.
* `minprob` - Minimum frequency of observations in terminal nodes.
* `maxheight` - Maximum height for the tree.

We'll use those but our fourth model we'll simply require a higher significance level for alpha2 and alpha4.

```{r best8, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
ctrl <- chaid_control(minsplit = 200, minprob = 0.05)
ctrl # notice the rest of the list is there at the default value
chaidattrit2 <- chaid(Attrition ~ ., data = newattrit, control = ctrl)
print(chaidattrit2)
plot(
  chaidattrit2,
  main = "minsplit = 200, minprob = 0.05",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3  )
)

ctrl <- chaid_control(maxheight = 3)
chaidattrit3 <- chaid(Attrition ~ ., data = newattrit, control = ctrl)
print(chaidattrit3)
plot(
  chaidattrit3,
  main = "maxheight = 3",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3  )
)

ctrl <- chaid_control(alpha2 = .01, alpha4 = .01)
chaidattrit4 <- chaid(Attrition ~ ., data = newattrit, control = ctrl)
print(chaidattrit4)
plot(
  chaidattrit4,
  main = "alpha2 = .01, alpha4 = .01",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3  )
)
```

Let me call your attention to `chaidattrit3` for a minute to highlight two important things. First it is a good picture of what we get for answer if we were to ask a question about what are the most important predictors, what variables should we focus on.  An important technical detail has emerged as well. Notice that when you look at inner node #3 that there is no technical reason why a node has to have a *binary* split in chaid.  As this example clearly shows node#3  leads to a three way split that is nodes #4-6.


## How good is our model?

So the obvious question is which model is best?  IMHO the joy of CHAID is in giving you a clear picture of what you would predict given the data and why.  Then of course there is the usual problem every data scientist has, which is, I have what I think is a great model.  How well will it generalize to new data?  Whether that's next years attrition numbers for the same company or say data from a different company.

But it's time to talk about accuracy and all the related ideas, so on with the show...

When it's all said and done we built a model called `chaidattrit1` to be able to predict or classify the 1,470 staff members.  Seems reasonable then that we can get back these predictions from the model for all 1,470 people and see how we did compared to the data we have about whether they attrited or not.  The print and plot commands sort of summarize that for us at the terminal node level with an error rate but all in all which of our four models is best?

The first step is to get the predictions for each model and put them somewhere.  For that we'll use the `predict` command.  If you inspect the object you create (in my case with a head command) you'll see it's a vector of factors where the attribute names is set to be the terminal node the prediction is associated with.  So `pmodel1 <- predict(chaidattrit1)` puts our predictions using the first model we built in a nice orderly fashion.  On the other side `newattrit$Attrition` has the actual outcome of whether the employee departed or not.

What we want is a comparison of how well we did. How often did we get it right or wrong?  Turns out what we need is called a confusion matrix. The `caret` package has a function called `confusionMatrix` that will give us what we want nicely formatted and printed.

There's a nice short summary of what is produced at this url [Confusion Matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/), so I won't even try to repeat that material. I'll just run the appropriate commands.  Later we'll revisit this topic to be more efficient.  For now I want to focus on the results.

```{r best9, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
# digress how accurate were we
pmodel1 <- predict(chaidattrit1)
head(pmodel1)
pmodel2 <- predict(chaidattrit2)
pmodel3 <- predict(chaidattrit3)
pmodel4 <- predict(chaidattrit4)
confusionMatrix(pmodel1, newattrit$Attrition)
confusionMatrix(pmodel2, newattrit$Attrition)
confusionMatrix(pmodel3, newattrit$Attrition)
confusionMatrix(pmodel4, newattrit$Attrition)
```

There we have it, four matrices, one for each of the models we made with the different control parameters.  It helpfully provides not just Accuracy but also other common measures you may be interested in.  I won't review them all that's why I provided [the link to a detailed description](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) of all the measures.  Before we leave the topic for a bit however, I do want to highlight a way you can use the `purrr` package to make your life a lot easier.  A special thanks to [Steven at MungeX-3D ](https://seslezak.github.io/) for his recent post on `purrr` which got me thinking about it.

We have 4 models so far (with more to come) we have the nice neat output from `caret` but honestly to compare values across the 4 models involves way too much scrolling back and forth right now.  Let's use `purrr` to create a nice neat dataframe.  `purrr`'s `map` command is like `lapply` from base R, designed to apply some operations or functions to a list of objects.  So what we'll do is as follows:

1.  Create a named list called `modellist` to point to our four existing models (perhaps at a latter date we'll start even earlier in our modelling process).
2.  It's a named list so we can name each model (for now with the accurate but uninteresting name Modelx)
3.  Pass the list using `map` to the `predict` function to generate our predictions
4.  Pipe `%>%` those results to the `confusionMatrix` function with `map`
5.  Pipe `%>%` the confusion matrix results to map_dfr. The results of confusionMattrix are actually a list of six items. The ones we want to capture are in `$overall` and `$byClass`. We grab them, transpose them, and make them into a dataframe then bind the two dataframes together so everything is neatly packaged. The `.id = ModelNumb` tells `map_dfr` to add an identifying column to the dataframe. It is populated with the name of the list item we passed in `modellist`.  Therefore the object CHAIDresults contains everything we might want to use to compare models in one neat dataframe.

The `kable` call is simply for your reading convenience.  Makes it a little easier to read than a traditional print call.

```{r best10, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
library(kableExtra)
modellist <- list(Model1 = chaidattrit1, Model2 = chaidattrit2, Model3 = chaidattrit3, Model4 = chaidattrit4)
CHAIDResults <- map(modellist, ~ predict(.x)) %>% 
                  map(~ confusionMatrix(newattrit$Attrition, .x)) %>%
                  map_dfr(~ cbind(as.data.frame(t(.x$overall)),as.data.frame(t(.x$byClass))), .id = "ModelNumb")
 kable(CHAIDResults, "html") %>% 
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                 font_size = 9)
```

One other thing I'll mention in passing is that the `partykit` package offers a way of assessing the relative importance of the variables in the model via the `varimp` command.  We'll come back to this concept of variable importance later but for now a simple example of text and plot output.

```{r best11}
sort(varimp(chaidattrit1), decreasing = TRUE)
plot(sort(varimp(chaidattrit1), decreasing = TRUE))
```

## What about those other variables?

But before we go much farther we should probably circle back and make use of all those variables that were coded as integers that we conveniently ignored in building our first four models.  Let's bring them into our model building activities and see what they can add to our understanding.  As a first step let's use `ggplot2` and take a look at their distribution using a density plot. 

```{r best12, fig.height=6, fig.width=9}
# Turning numeric variables into factors
## what do they look like
attrition %>%
  select_if(is.numeric) %>%
  gather(metric, value) %>%
  ggplot(aes(value, fill = metric)) +
  geom_density(show.legend = FALSE) +
  facet_wrap( ~ metric, scales = "free")
```

Well other than `Age` very few of those variables appear to have especially normal distributions.  That's okay we're going to wind up cutting them up into factors anyway.  The only question is what are the best cut-points to use?  In base R the `cut` function default is equal intervals (distances along the x axis). You can also specify your own cutpoints and your own labels as shown below.

```{r best13}
table(cut(attrition$YearsWithCurrManager, breaks = 5))
table(attrition$YearsSinceLastPromotion)
table(cut(
  attrition$YearsSinceLastPromotion,
  breaks = c(-1, 0.9, 1.9, 2.9, 30),
  labels = c("Less than 1", "1", "2", "More than 2")
))
```

`ggplot2` has three helper functions I prefer to use: `cut_interval`, `cut_number`, and `cut_width`. `cut_interval` makes n groups with equal range, `cut_number` makes n groups with (approximately) equal numbers of observations, and `cut_width` makes groups of a fixed specified width.  As we think about moving the numeric variables into factors any of these might be a viable alternative.

```{r best14}
# cut_interval makes n groups with equal range
table(cut_interval(attrition$YearsWithCurrManager, n = 5)) 
# cut_number makes n groups with (approximately) equal numbers of observations
table(cut_number(attrition$YearsWithCurrManager, n = 5)) 
# cut_width makes groups of width width
table(cut_width(attrition$YearsWithCurrManager, width = 2)) 
```

For the sake of our current example let's say that I would like to focus on groups of more or less equal size which means that I would need to apply `cut_number` to each of the 12 variables under discussion.  I'm not enamored of running the function 12 times though so I would prefer to wrap it in a `mutate_if` statement. If the variable is numeric then apply `cut_number` with n=5.

The problem is that `cut_number` will error out if it doesn't think there are enough values to produce the bins you requested. So...

```{r best15, eval=FALSE}
cut_number(attrition$YearsWithCurrManager, n = 6)
# Error: Insufficient data values to produce 6 bins.
cut_number(attrition$YearsSinceLastPromotion, n = 4)
# Error: Insufficient data values to produce 4 bins.
attrition %>% 
  mutate_if(is.numeric, funs(cut_number(., n=5)))
# Error in mutate_impl(.data, dots) : 
#   Evaluation error: Insufficient data values to produce 5 bins..
```

A little sleuthing reveals that there is one variable among the 12 that has too few values for the `cut_number` function to work.  That variable is `YearsSinceLastPromotion`.  Let's try what we would like but explicitly `select` out that variable.

```{r best16, eval=TRUE}
attrition %>% 
  select(-YearsSinceLastPromotion) %>% 
  mutate_if(is.numeric, funs(cut_number(., n=5))) %>% head
```

Yes that appears to be it. So let's manually cut it into 4 groups and then apply the 5 grouping code to the other 11 variables.  Once we have accomplished that we can run the same `newattrit <- attrition %>% select_if(is.factor)` we ran earlier to produce a `newattrit` dataframe we can work with.

```{r best17, eval=TRUE}
attrition$YearsSinceLastPromotion <- cut(
  attrition$YearsSinceLastPromotion,
  breaks = c(-1, 0.9, 1.9, 2.9, 30),
  labels = c("Less than 1", "1", "2", "More than 2")
)

attrition <- attrition %>% 
                  mutate_if(is.numeric, funs(cut_number(., n=5)))
summary(attrition)

newattrit <- attrition %>% 
  select_if(is.factor)
dim(newattrit)
```

Now we have `newattrit` with all 30 predictor variables. We will simply repeat the process we used earlier to develop 4 new models.  

```{r best18, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
 
# Repeat to produce models 5-8
chaidattrit5 <- chaid(Attrition ~ ., data = newattrit)
print(chaidattrit5)
plot(
  chaidattrit5,
  main = "Default control sliced numerics",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3,
    fontsize = 8
  )
)

ctrl <- chaid_control(minsplit = 200, minprob = 0.05)
chaidattrit6 <- chaid(Attrition ~ ., data = newattrit, control = ctrl)
print(chaidattrit6)
plot(
  chaidattrit6,
  main = "minsplit = 200, minprob = 0.05",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3,
    fontsize = 8
  )
)

ctrl <- chaid_control(maxheight = 3)
chaidattrit7 <- chaid(Attrition ~ ., data = newattrit, control = ctrl)
print(chaidattrit7)
plot(
  chaidattrit7,
  main = "maxheight = 3",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3,
    fontsize = 8
  )
)

ctrl <- chaid_control(alpha2 = .01, alpha4 = .01)
chaidattrit8 <- chaid(Attrition ~ ., data = newattrit, control = ctrl)
print(chaidattrit8)
plot(
  chaidattrit8,
  main = "alpha2 = .01, alpha4 = .01",
  gp = gpar(
    col = "blue",
    lty = "solid",
    lwd = 3,
    fontsize = 8
  )
)
```

As we did earlier we'll also repeat the steps necessary to build a table of results.

```{r best19, fig.height=10, fig.width=20, dpi=90, out.width="900px"}
modellist <- list(Model1 = chaidattrit1, 
                  Model2 = chaidattrit2, 
                  Model3 = chaidattrit3, 
                  Model4 = chaidattrit4, 
                  Model5 = chaidattrit5, 
                  Model6 = chaidattrit6, 
                  Model7 = chaidattrit7, 
                  Model8 = chaidattrit8)
CHAIDResults <- map(modellist, ~ predict(.x)) %>% 
  map(~ confusionMatrix(newattrit$Attrition, .x)) %>%
  map_dfr(~ cbind(as.data.frame(t(.x$overall)),as.data.frame(t(.x$byClass))), .id = "ModelNumb")
kable(CHAIDResults, "html") %>% 
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                 font_size = 10)
```

You can clearly see that `Overtime` remains the first cut in our tree structure but that now other variables have started to influence our model as well, such as how long they've worked for us and their age.  You can see from the table that model #5 is apparently the most accurate now. Not by a huge amount but apparently these numeric variables we ignored at first pass do matter at least to some degree.

## Not done yet

I'm not going to dwell on the current results too much they are simply for an example and in my next post I'd like to spend some time on over-fitting and cross validation.

I hope you've found this useful.  I am always open to comments, corrections and suggestions.

Chuck (ibecav at gmail dot com)

### License
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.


